# AI-ML-Internship-Task5: Decision Trees & Random Forests 🌳🌲

## 📖 Description
Day-5 Task of my **AI & ML Internship**: Implementation of **Decision Tree and Random Forest classifiers** on the Heart Disease dataset.  
This notebook covers tree-based models, overfitting analysis, ensemble learning, and feature importance interpretation.

---

## 🚀 Objectives
- Train a **Decision Tree Classifier** and visualize the tree.  
- Analyze **overfitting** and control depth.  
- Train a **Random Forest Classifier** and compare with Decision Tree.  
- Interpret **feature importances**.  
- Evaluate models using **cross-validation**.  

---

## 🛠 Tools & Libraries
- Python 3  
- Pandas, NumPy  
- Matplotlib, Seaborn  
- Scikit-learn  
- Graphviz (for tree visualization)  

---

## 📂 Repository Contents
AI-ML-Internship-Task5/
│
├── task5.ipynb # Colab notebook with implementation
├── README.md # Documentation

---

## 📊 Dataset
- **Dataset Used**: [Heart Disease Dataset (Kaggle)](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)  
- Target variable: `target` (1 = Heart Disease, 0 = No Heart Disease)  

---

## 🔎 Implementation Workflow
1. Data Import & Exploration  
2. Train Decision Tree Classifier  
3. Visualize Decision Tree (Graphviz)  
4. Analyze Overfitting (tuning `max_depth`)  
5. Train Random Forest Classifier  
6. Compare accuracy between Tree & Forest  
7. Interpret Feature Importances  
8. Evaluate using Cross-Validation  
9. Observations & Interview Prep Notes  

---

## 📈 Results (Sample)
- **Decision Tree Accuracy**: ~80% (depth-limited)  
- **Random Forest Accuracy**: ~85-90%  
- **Feature Importances**: Age, Cholesterol, Thal, etc.  

---

## 📷 Visual Outputs
- Decision Tree Plot  
- Feature Importance Bar Chart  
- Accuracy Comparison Graph  

*(Add screenshots here after running notebook)*  

---

## 🧾 Key Learnings
- How Decision Trees split data using **entropy & information gain**.  
- Overfitting occurs when depth is too high → controlled with `max_depth`.  
- Random Forest improves accuracy via **bagging & ensembling**.  
- Feature importance helps identify key predictors.  

---

## 👨‍💻 Author
**Balaji**  
B.Tech CSE (AI-ML), Parul University  
